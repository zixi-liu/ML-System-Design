## A Survey of Large Language Models

[A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223)

#### Introduction

**Task solving capacity of LLMs**

- Statistical LM: Specific task helper
- Neural LM: Task-agnostic feature learner (Word2vec etc.)
- Transferable NLP task solver - Pre-trained LM (ELMO, BERT, GPT-1/2)
- General-purpose task solver - LLM (GPT-3/4, ChatGPT, Claude)

**Three major differences between LLMs and PLMs**

- LLMs display some surprising emergent abilities that may not be observed in previous smaller PLMs.
- Unlike small PLMs, the major approach to accessing LLMs is through the prompting interface (e.g., GPT-4 API).
- The training of LLMs requires extensive practical experiences in large-scale data processing and distributed parallel training.

**Recent advances in LLMs from four major aspects**

- Pre-training (how to pretrain a capable LLM)
- Adaptation (how to effectively adapt pre-trained LLMs for better use)
- Utilization (how to use LLMs for solving various downstream tasks)
- Capability evaluation (how to evaluate the abilities of LLMs and existing empirical findings). 
