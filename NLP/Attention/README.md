
**综述**

[一文浅析attention](https://zhuanlan.zhihu.com/p/453294361)
- 传统的attention
  - [[ICLR 2015] Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473)
    - bahdanau attention / additive attention
  - [[EMNLP 2015] Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)
    - Luong attention / multiplicative attention
  - [Show, attend and tell: neural image caption generation with visual attention](https://dl.acm.org/doi/10.5555/3045118.3045336)
  - - hard/soft attention
